{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Install visualization libraries\n\n!pip install --upgrade pixiedust\n!pip install --upgrade bokeh\n\n# pixiedust.installPackage(\"cloudant-labs:spark-cloudant:2.0.0-s_2.11\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "# Use visualization libraries\n\nimport pixiedust\nimport bokeh"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a Spark dataframe from Cloudant\n\nsparkSession = SQLContext.getOrCreate(sc).sparkSession\n\ncloudantdata=sparkSession.read.format(\"com.cloudant.spark\")\\\n  .option(\"cloudant.host\",credentials_1['host'])\\\n  .option(\"cloudant.username\", credentials_1['username'])\\\n  .option(\"cloudant.password\", credentials_1['password'])\\\n  .option(\"jsonstore.rdd.partitions\", \"1\")\\\n  .option(\"inferSchema\", \"True\")\\\n  .load(\"zzshake\")\n\n# Persist dataframe and show structure\n\ncloudantdata.createOrReplaceTempView(\"dataframe\")\ncloudantdata.printSchema"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Use SQL to query the dataframe and show aggregation of energy\n\nsqldata = \"select cast (sum(sqrt((AX * AX) + (AY * AY) + (AZ * AZ))) as integer) as EN, ID from dataframe group by ID order by EN desc\"\nmodeldata = spark.sql(sqldata)\nmodeldata.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "mpld3": "false", 
                        "aggregation": "SUM", 
                        "rowCount": "500", 
                        "handlerId": "barChart", 
                        "valueFields": "EN", 
                        "rendererId": "matplotlib", 
                        "sortby": "Values DESC", 
                        "timeseries": "false", 
                        "keyFields": "ID"
                    }
                }
            }, 
            "outputs": [], 
            "source": "# Use PixieDust to visualize the results\n\ndisplay(modeldata)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "lineChartType": "grouped", 
                        "mpld3": "false", 
                        "aggregation": "SUM", 
                        "logx": "false", 
                        "logy": "false", 
                        "handlerId": "lineChart", 
                        "valueFields": "AX,AY,AZ", 
                        "rendererId": "matplotlib", 
                        "keyFields": "TS"
                    }
                }, 
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "# Access the full dataset and show timeseries\n\nsqldata = \"select TS, ID, AX, AY, AZ, OA, OB, OG, sqrt((AX * AX) + (AY * AY) + (AZ * AZ)) as EN from dataframe\"\nmodeldata = spark.sql(sqldata)\ndisplay(modeldata)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "lineChartType": "grouped", 
                        "mpld3": "false", 
                        "aggregation": "SUM", 
                        "logx": "false", 
                        "logy": "false", 
                        "clusterby": "SENSORID", 
                        "handlerId": "lineChart", 
                        "valueFields": "OA,OB,OG", 
                        "rendererId": "matplotlib", 
                        "timeseries": "false", 
                        "keyFields": "TS"
                    }
                }, 
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "# Another visualization of time series\n\ndisplay(modeldata)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Check structure of Python object\n\nmodeldata"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "tableView"
                    }
                }
            }, 
            "outputs": [], 
            "source": "# Preview data\n\ndisplay(modeldata)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Check structure of dataframe\n\nmodeldata.printSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Preview data\n\nmodeldata.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Basic statistics\n\nmodeldata.describe().show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Number of records\n\nmodeldata.count()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# 50/50 split into training and test set\n\nsplit_data = modeldata.randomSplit([0.5, 0.5], 0)\ntrain_data = split_data[0]\ntest_data  = split_data[1]\n\nprint \"Number of training records : \" + str(train_data.count())\nprint \"Number of testing records  : \" + str(test_data.count())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Preprocessing and machine learning functions\n\nfrom pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler\n\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import GBTClassifier\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nfrom pyspark.ml import Pipeline, Model"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Convert target to numerical and check contents\n\nstringIndexer_label = StringIndexer(inputCol = \"ID\", outputCol = \"label\").fit(modeldata)\nstringIndexer_label.labels"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Define model input\n\nvectorAssembler_features = VectorAssembler(inputCols=[\"AX\", \"AY\", \"AZ\", \"OA\", \"OB\", \"OG\"], outputCol = \"features\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Convert predictoins back to labels\n\nlabelConverter = IndexToString(inputCol = \"prediction\", outputCol = \"predlabel\", labels = stringIndexer_label.labels)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Define algorithm to use in modeling\n\ncf = RandomForestClassifier(labelCol = \"label\", featuresCol = \"features\")\n\n# cf = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Build the modeling pipeline\n\npipeline_cf = Pipeline(stages = [stringIndexer_label, vectorAssembler_features, cf, labelConverter])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Build a model\n\nmodel_cf = pipeline_cf.fit(train_data)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Run the test data through the pipeline and check accuracy\n\npredictions = model_cf.transform(test_data)\nevaluator = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol = \"prediction\", metricName = \"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(\"Accuracy = %g\" % accuracy)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "# Sample the prediction output\n\npredictions.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "kind": "kde", 
                        "mpld3": "false", 
                        "dynamicfilter": "label", 
                        "rowCount": "10000", 
                        "handlerId": "scatterPlot", 
                        "valueFields": "prediction", 
                        "rendererId": "seaborn", 
                        "keyFields": "label"
                    }
                }
            }, 
            "outputs": [], 
            "source": "# Visualize the classification\n\ndisplay(predictions)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.14", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}